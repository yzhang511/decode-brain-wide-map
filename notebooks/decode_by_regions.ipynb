{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "34812d8d-a1c4-4cab-a1c1-96b04fa8992b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from one.api import ONE\n",
    "from brainbox.io.one import SessionLoader\n",
    "from iblatlas.regions import BrainRegions\n",
    "\n",
    "from sklearn import linear_model as sklm\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, r2_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from behavior_models.utils import format_data as format_data_mut\n",
    "from behavior_models.utils import format_input as format_input_mut\n",
    "\n",
    "from brainwidemap.bwm_loading import load_good_units, load_all_units, load_trials_and_mask, merge_probes\n",
    "from brainwidemap.decoding.functions.process_targets import load_behavior\n",
    "from brainwidemap.decoding.settings_for_BWM_figure.settings_choice import params\n",
    "from brainwidemap.decoding.settings_for_BWM_figure.settings_choice import RESULTS_DIR\n",
    "\n",
    "from brainwidemap.decoding.functions.balancedweightings import balanced_weighting\n",
    "from brainwidemap.decoding.functions.process_inputs import (\n",
    "    build_predictor_matrix,\n",
    "    select_ephys_regions,\n",
    "    preprocess_ephys\n",
    ")\n",
    "from brainwidemap.decoding.functions.process_targets import (\n",
    "    compute_beh_target,\n",
    "    compute_target_mask,\n",
    "    transform_data_for_decoding,\n",
    "    logisticreg_criteria,\n",
    "    get_target_data_per_trial_wrapper,\n",
    "    check_bhv_fit_exists,\n",
    "    optimal_Bayesian\n",
    ")\n",
    "from brainwidemap.decoding.functions.utils import save_region_results, get_save_path\n",
    "from brainwidemap.decoding.functions.nulldistributions import generate_null_distribution_session\n",
    "from brainwidemap.decoding.functions.decoding import decode_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c4976e-a2b4-4ab6-8576-f220e9bd572f",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06554af5-db49-4411-96d4-08bedf04551e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params['behfit_path'] = RESULTS_DIR.joinpath('decoding', 'results', 'behavioral')\n",
    "params['behfit_path'].mkdir(parents=True, exist_ok=True)\n",
    "params['neuralfit_path'] = RESULTS_DIR.joinpath('decoding', 'results', 'neural')\n",
    "params['neuralfit_path'].mkdir(parents=True, exist_ok=True)\n",
    "params['add_to_saving_path'] = (f\"_binsize={1000 * params['binsize']}_lags={params['n_bins_lag']}_\"\n",
    "                                f\"mergedProbes_{params['merged_probes']}\")\n",
    "imposter_file = RESULTS_DIR.joinpath('decoding', f\"imposterSessions_{params['target']}.pqt\")\n",
    "bwm_session_file = RESULTS_DIR.joinpath('decoding', 'bwm_cache_sessions.pqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1316757c-d3ca-48bc-9bf4-67c06d296821",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "firstMovement_times\n"
     ]
    }
   ],
   "source": [
    "# params[\"align_time\"] = \"stimOn_times\"\n",
    "params[\"align_time\"] = \"firstMovement_times\"\n",
    "print(params[\"align_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "910af896-fb9c-45e8-9ac1-7856e7c304c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.1, 0.0)\n"
     ]
    }
   ],
   "source": [
    "# params[\"time_window\"] = (-0.5, 1.)\n",
    "params[\"time_window\"] = (-.1, .0)\n",
    "print(params[\"time_window\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6993fba-3688-4180-88e8-55779486553c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "params[\"binsize\"] = 0.1\n",
    "# params[\"binsize\"] = 0.05\n",
    "print(params[\"binsize\"])\n",
    "params['n_pseudo'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f477db7-e72d-453a-b05e-488ebbcaecd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "one = ONE(base_url=\"https://openalyx.internationalbrainlab.org\", mode='remote')\n",
    "bwm_df = pd.read_parquet(bwm_session_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b28730ab-0c65-425a-83bb-47bb1b6c558f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "roi = \"po\"\n",
    "pids_per_region = one.search_insertions(atlas_acronym=[roi], query_type='remote')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "02c6eba6-b886-4db5-907f-e3dd5c93d4ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>eid</th>\n",
       "      <th>probe_name</th>\n",
       "      <th>session_number</th>\n",
       "      <th>date</th>\n",
       "      <th>subject</th>\n",
       "      <th>lab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>5810514e-2a86-4a34-b7bd-1e4b0b601295</td>\n",
       "      <td>5386aba9-9b97-4557-abcd-abc2da66b863</td>\n",
       "      <td>probe00</td>\n",
       "      <td>2</td>\n",
       "      <td>2020-02-19</td>\n",
       "      <td>CSHL052</td>\n",
       "      <td>churchlandlab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>c2363000-27a6-461e-940b-15f681496ed8</td>\n",
       "      <td>4fa70097-8101-4f10-b585-db39429c5ed0</td>\n",
       "      <td>probe00</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-21</td>\n",
       "      <td>DY_010</td>\n",
       "      <td>danlab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>3eb6e6e0-8a57-49d6-b7c9-f39d5834e682</td>\n",
       "      <td>0802ced5-33a3-405e-8336-b65ebc5cb07c</td>\n",
       "      <td>probe01</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-06-23</td>\n",
       "      <td>ZFM-02373</td>\n",
       "      <td>mainenlab</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      pid  \\\n",
       "73   5810514e-2a86-4a34-b7bd-1e4b0b601295   \n",
       "224  c2363000-27a6-461e-940b-15f681496ed8   \n",
       "355  3eb6e6e0-8a57-49d6-b7c9-f39d5834e682   \n",
       "\n",
       "                                      eid probe_name  session_number  \\\n",
       "73   5386aba9-9b97-4557-abcd-abc2da66b863    probe00               2   \n",
       "224  4fa70097-8101-4f10-b585-db39429c5ed0    probe00               1   \n",
       "355  0802ced5-33a3-405e-8336-b65ebc5cb07c    probe01               1   \n",
       "\n",
       "           date    subject            lab  \n",
       "73   2020-02-19    CSHL052  churchlandlab  \n",
       "224  2020-01-21     DY_010         danlab  \n",
       "355  2021-06-23  ZFM-02373      mainenlab  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bwm_df = bwm_df.loc[bwm_df.pid.isin(pids_per_region)]\n",
    "bwm_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f9a4cd4d-d936-4607-8620-5eb3cbd5930d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yizi/decode-brain-wide-map/results/po/firstMovement_times_-0.1_0.0_0.1_good_units.npy\n"
     ]
    }
   ],
   "source": [
    "unit_type = \"good_units\"\n",
    "dir_path = Path(\"/home/yizi/decode-brain-wide-map/results\")\n",
    "save_path = dir_path/roi/f'{params[\"align_time\"]}_{params[\"time_window\"][0]}_{params[\"time_window\"][1]}_{params[\"binsize\"]}_{unit_type}.npy'\n",
    "print(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baece939-7e7a-49c5-8430-8dc7d335d44b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running merged probes for session eid: a8a8af78-16de-4841-ab07-fde4b5281a03\n"
     ]
    }
   ],
   "source": [
    "metrics_by_eids, filenames_by_eids = {}, {}\n",
    "for idx in range(bwm_df.shape[0]):\n",
    "\n",
    "    job_repeat = 0 \n",
    "    pseudo_ids = np.arange(job_repeat * params['n_pseudo_per_job'], (job_repeat + 1) * params['n_pseudo_per_job']) + 1\n",
    "    if 1 in pseudo_ids:\n",
    "        pseudo_ids = np.concatenate((-np.ones(1), pseudo_ids)).astype('int64')\n",
    "    if pseudo_ids[0] > params['n_pseudo']:\n",
    "        print(f\"ended job because this job_repeat ({job_repeat}) does not include any pseudo sessions < {params['n_pseudo']}\")\n",
    "        exit()\n",
    "    if pseudo_ids[-1] > params['n_pseudo']:\n",
    "        print(f\"truncated job because this job_repeat ({job_repeat}) includes more than {params['n_pseudo']} pseudo sessions\")\n",
    "        pseudo_ids = pseudo_ids[pseudo_ids <= params['n_pseudo']]\n",
    "\n",
    "    if params['merged_probes']:\n",
    "        eid = bwm_df['eid'].unique()[idx]\n",
    "        tmp_df = bwm_df.set_index(['eid', 'subject']).xs(eid, level='eid')\n",
    "        subject = tmp_df.index[0]\n",
    "        pids = tmp_df['pid'].to_list()  # Select all probes of this session\n",
    "        probe_names = tmp_df['probe_name'].to_list()\n",
    "        print(f\"Running merged probes for session eid: {eid}\")\n",
    "    else:\n",
    "        eid = bwm_df.iloc[idx]['eid']\n",
    "        subject = bwm_df.iloc[idx]['subject']\n",
    "        pid = bwm_df.iloc[idx]['pid']\n",
    "        probe_name = bwm_df.iloc[idx]['probe_name']\n",
    "        print(f\"Running probe pid: {pid}\")\n",
    "\n",
    "    sess_loader = SessionLoader(one, eid)\n",
    "    sess_loader.load_trials()\n",
    "\n",
    "    trials_df, trials_mask = load_trials_and_mask(\n",
    "        one=one, eid=eid, sess_loader=sess_loader, min_rt=params['min_rt'], max_rt=params['max_rt'],\n",
    "        min_trial_len=params['min_len'], max_trial_len=params['max_len'],\n",
    "        exclude_nochoice=True, exclude_unbiased=params['exclude_unbiased_trials'])\n",
    "    _, trials_mask_without_minrt = load_trials_and_mask(\n",
    "        one=one, eid=eid, sess_loader=sess_loader, min_rt=None, max_rt=params['max_rt'],\n",
    "        min_trial_len=params['min_len'], max_trial_len=params['max_len'],\n",
    "        exclude_nochoice=True, exclude_unbiased=params['exclude_unbiased_trials'])\n",
    "    _, trials_mask_without_maxrt = load_trials_and_mask(\n",
    "        one=one, eid=eid, sess_loader=sess_loader, min_rt=params['min_rt'], max_rt=None,\n",
    "        min_trial_len=params['min_len'], max_trial_len=params['max_len'],\n",
    "        exclude_nochoice=True, exclude_unbiased=params['exclude_unbiased_trials'])\n",
    "    _, trials_mask_withonly_nochoice = load_trials_and_mask(\n",
    "        one=one, eid=eid, sess_loader=sess_loader, min_rt=None, max_rt=None,\n",
    "        min_trial_len=None, max_trial_len=None,\n",
    "        exclude_nochoice=True, exclude_unbiased=False)\n",
    "\n",
    "    params['trials_mask_diagnostics'] = [trials_mask,\n",
    "                                         trials_mask_without_minrt,\n",
    "                                         trials_mask_without_maxrt,\n",
    "                                         trials_mask_withonly_nochoice]\n",
    "\n",
    "    if params['target'] in ['wheel-vel', 'wheel-speed', 'l-whisker-me', 'r-whisker-me']:\n",
    "        # load target data\n",
    "        dlc_dict = load_behavior(params['target'], sess_loader)\n",
    "        # load imposter sessions\n",
    "        params['imposter_df'] = pd.read_parquet(imposter_file) if params['n_pseudo'] > 0 else None\n",
    "    else:\n",
    "        dlc_dict = None\n",
    "        params['imposter_df'] = None\n",
    "\n",
    "    if params['merged_probes']:\n",
    "        clusters_list = []\n",
    "        spikes_list = []\n",
    "        for pid, probe_name in zip(pids, probe_names):\n",
    "            tmp_spikes, tmp_clusters = load_good_units(one, pid, eid=eid, pname=probe_name)\n",
    "            # tmp_spikes, tmp_clusters = load_all_units(one, pid, eid=eid, pname=probe_name)\n",
    "            tmp_clusters['pid'] = pid\n",
    "            spikes_list.append(tmp_spikes)\n",
    "            clusters_list.append(tmp_clusters)\n",
    "        spikes, clusters = merge_probes(spikes_list, clusters_list)\n",
    "    else:\n",
    "        spikes, clusters = load_good_units(one, pid, eid=eid, pname=probe_name)\n",
    "        # spikes, clusters = load_all_units(one, pid, eid=eid, pname=probe_name)\n",
    "\n",
    "    neural_dict = {\n",
    "        'spk_times': spikes['times'],\n",
    "        'spk_clu': spikes['clusters'],\n",
    "        'clu_regions': clusters['acronym'],\n",
    "        'clu_qc': {k: np.asarray(v) for k, v in clusters.to_dict('list').items()},\n",
    "        'clu_df': clusters\n",
    "    }\n",
    "\n",
    "    metadata = {\n",
    "        'subject': subject,\n",
    "        'eid': eid,\n",
    "        'probe_name': probe_name\n",
    "    }\n",
    "    \n",
    "    kwargs = params\n",
    "    kwargs['n_runs'] = 1\n",
    "    kwargs['n_bins_lag'] = 0\n",
    "    \n",
    "    print(f'Working on eid: {metadata[\"eid\"]}')\n",
    "    filenames = []  # this will contain paths to saved decoding results for this eid\n",
    "\n",
    "    if kwargs['use_imposter_session'] and not kwargs['stitching_for_imposter_session']:\n",
    "        trials_df = trials_df[:int(kwargs['max_number_trials_when_no_stitching_for_imposter_session'])]\n",
    "\n",
    "    if 0 in pseudo_ids:\n",
    "        raise ValueError(\n",
    "            'pseudo id can be -1 (actual session) or strictly greater than 0 (pseudo session)')\n",
    "\n",
    "    if not np.all(np.sort(pseudo_ids) == pseudo_ids):\n",
    "        raise ValueError('pseudo_ids must be sorted')\n",
    "\n",
    "    if kwargs['model'] == optimal_Bayesian and np.any(trials_df.probabilityLeft.values[:90] != 0.5):\n",
    "        raise ValueError(\n",
    "            'The optimal Bayesian model assumes 90 unbiased trials at the beginning of the '\n",
    "            'session, which is not the case here.')\n",
    "\n",
    "    # check if is trained\n",
    "    eids_train = (\n",
    "        [metadata['eid']] if 'eids_train' not in metadata.keys() else metadata['eids_train'])\n",
    "    if 'eids_train' not in metadata.keys():\n",
    "        metadata['eids_train'] = eids_train\n",
    "    elif metadata['eids_train'] != eids_train:\n",
    "        raise ValueError(\n",
    "            'eids_train are not supported yet. If you do not understand this error, '\n",
    "            'just take out the eids_train key in the metadata to solve it')\n",
    "\n",
    "    if isinstance(kwargs['model'], str):\n",
    "        import pickle\n",
    "        from braindelphi.params import INTER_INDIVIDUAL_PATH\n",
    "        inter_individual = pickle.load(open(INTER_INDIVIDUAL_PATH.joinpath(kwargs['model']), 'rb'))\n",
    "        if metadata['eid'] not in inter_individual.keys():\n",
    "            logging.exception('no inter individual model found')\n",
    "            print(filenames)\n",
    "        inter_indiv_model_specifications = inter_individual[metadata['eid']]\n",
    "        print('winning interindividual model is %s' % inter_indiv_model_specifications['model_name'])\n",
    "        if inter_indiv_model_specifications['model_name'] not in kwargs['modeldispatcher'].values():\n",
    "            logging.exception('winning inter individual model is LeftKernel or RightKernel')\n",
    "            print(filenames)\n",
    "        kwargs['model'] = {v: k for k, v in kwargs['modeldispatcher'].items()}[inter_indiv_model_specifications['model_name']]\n",
    "        kwargs['model_parameters'] = inter_indiv_model_specifications['model_parameters']\n",
    "    else:\n",
    "        kwargs['model_parameters'] = None\n",
    "        # train model if not trained already\n",
    "        if kwargs['model'] != optimal_Bayesian and kwargs['model'] is not None:\n",
    "            side, stim, act, _ = format_data_mut(trials_df)\n",
    "            stimuli, actions, stim_side = format_input_mut([stim], [act], [side])\n",
    "            behmodel = kwargs['model'](\n",
    "                kwargs['behfit_path'], np.array(metadata['eids_train']), metadata['subject'],\n",
    "                actions, stimuli, trials_df, stim_side, single_zeta=True)\n",
    "            istrained, _ = check_bhv_fit_exists(\n",
    "                metadata['subject'], kwargs['model'], metadata['eids_train'],\n",
    "                kwargs['behfit_path'], modeldispatcher=kwargs['modeldispatcher'], single_zeta=True)\n",
    "            if not istrained:\n",
    "                behmodel.load_or_train(remove_old=False)\n",
    "\n",
    "    if kwargs['balanced_weight'] and kwargs['balanced_continuous_target']:\n",
    "        raise NotImplementedError(\"see tag `decoding_biasCWnull` for a previous implementation.\")\n",
    "    else:\n",
    "        target_distribution = None\n",
    "\n",
    "    # get target values\n",
    "    if kwargs['target'] in ['pLeft', 'signcont', 'strengthcont', 'choice', 'feedback']:\n",
    "        target_vals_list, target_vals_to_mask = compute_beh_target(\n",
    "            trials_df, metadata, return_raw=True, **kwargs)\n",
    "        target_mask = compute_target_mask(\n",
    "            target_vals_to_mask, kwargs['exclude_trials_within_values'])\n",
    "\n",
    "    else:\n",
    "        if dlc_dict is None or dlc_dict['times'] is None or dlc_dict['values'] is None:\n",
    "            raise ValueError('dlc_dict does not contain any data')\n",
    "        _, target_vals_list, target_mask = get_target_data_per_trial_wrapper(\n",
    "            target_times=dlc_dict['times'],\n",
    "            target_vals=dlc_dict['values'],\n",
    "            trials_df=trials_df,\n",
    "            align_event=kwargs['align_time'],\n",
    "            align_interval=kwargs['time_window'],\n",
    "            binsize=kwargs['binsize'])\n",
    "\n",
    "    mask = trials_mask & target_mask\n",
    "\n",
    "    if sum(mask) <= kwargs['min_behav_trials']:\n",
    "        msg = 'session contains %i trials, below the threshold of %i' % (\n",
    "            sum(mask), kwargs['min_behav_trials'])\n",
    "        logging.exception(msg)\n",
    "        print(filenames)\n",
    "        \n",
    "    # select brain regions from beryl atlas to loop over\n",
    "    brainreg = BrainRegions()\n",
    "    beryl_reg = brainreg.acronym2acronym(neural_dict['clu_regions'], mapping='Beryl')\n",
    "    regions = (\n",
    "        [[k] for k in np.unique(beryl_reg)] if kwargs['single_region'] else [np.unique(beryl_reg)])\n",
    "    \n",
    "    roi_idx = np.argwhere(np.array([region[0].lower().find(roi) for region in regions]) == 0).astype(int)\n",
    "    regions = regions[roi_idx.item()]\n",
    "    for region in tqdm(regions, desc='Region: ', leave=False):\n",
    "        \n",
    "        print(regions[0])\n",
    "        # pull spikes from this region out of the neural data\n",
    "        reg_clu_ids = select_ephys_regions(neural_dict, beryl_reg, region, **kwargs)\n",
    "\n",
    "        # skip region if there are not enough units\n",
    "        n_units = len(reg_clu_ids)\n",
    "        if n_units < kwargs['min_units']:\n",
    "            continue\n",
    "\n",
    "        # bin spikes from this region for each trial\n",
    "        msub_binned, cl_inds_used = preprocess_ephys(reg_clu_ids, neural_dict, trials_df, **kwargs)\n",
    "        cl_uuids_used = list(neural_dict['clu_df'].iloc[cl_inds_used]['uuids'])\n",
    "\n",
    "        # make design matrix\n",
    "        bins_per_trial = msub_binned[0].shape[0]\n",
    "        Xs = (\n",
    "            msub_binned if bins_per_trial == 1\n",
    "            else [build_predictor_matrix(s, kwargs['n_bins_lag']) for s in msub_binned]\n",
    "        )\n",
    "\n",
    "        for pseudo_id in pseudo_ids:\n",
    "            fit_results = []\n",
    "\n",
    "            # save out decoding results\n",
    "            save_path = get_save_path(\n",
    "                pseudo_id, metadata['subject'], metadata['eid'], 'ephys',\n",
    "                probe=metadata['probe_name'],\n",
    "                region=str(np.squeeze(region)) if kwargs['single_region'] else 'allRegions',\n",
    "                output_path=kwargs['neuralfit_path'],\n",
    "                time_window=kwargs['time_window'],\n",
    "                date=kwargs['date'],\n",
    "                target=kwargs['target'],\n",
    "                add_to_saving_path=kwargs['add_to_saving_path']\n",
    "            )\n",
    "            if os.path.exists(save_path):\n",
    "                print(\n",
    "                    f'results for region {region}, pseudo_id {pseudo_id} already exist at '\n",
    "                    f'{save_path}')\n",
    "                filenames.append(save_path)\n",
    "                continue\n",
    "\n",
    "            # get data matrix and target, resampling when there are <3 incorrect trials\n",
    "\n",
    "            # create pseudo/imposter session when necessary, corresponding mask, and data matrix\n",
    "            if pseudo_id > 0:\n",
    "                ys_wmask = None\n",
    "                Xs_wmask = None\n",
    "                sample_pseudo_count = 0\n",
    "                while (kwargs['estimator']==sklm.LogisticRegression and (not logisticreg_criteria(ys_wmask))) or (ys_wmask is None):\n",
    "                    assert sample_pseudo_count < 100 # must be a reasonable number of sample or else something is wrong\n",
    "                    sample_pseudo_count += 1\n",
    "                    if bins_per_trial == 1:\n",
    "                        controlsess_df = generate_null_distribution_session(\n",
    "                            trials_df, metadata, **kwargs)\n",
    "                        controltarget_vals_list, controltarget_vals_to_mask = compute_beh_target(\n",
    "                            controlsess_df, metadata, return_raw=True, **kwargs)\n",
    "                        controltarget_mask = compute_target_mask(\n",
    "                            controltarget_vals_to_mask, kwargs['exclude_trials_within_values'])\n",
    "                        control_mask = trials_mask & controltarget_mask\n",
    "                    else:\n",
    "                        imposter_df = kwargs['imposter_df'].copy()\n",
    "                        # remove current eid from imposter sessions\n",
    "                        df_clean = imposter_df[imposter_df.eid != metadata['eid']].reset_index()\n",
    "                        # randomly select imposter trial to start sequence\n",
    "                        n_trials = trials_df.index.size\n",
    "                        total_imposter_trials = df_clean.shape[0]\n",
    "                        idx_beg = np.random.choice(total_imposter_trials - n_trials)\n",
    "                        controlsess_df = df_clean.iloc[idx_beg:idx_beg + n_trials]\n",
    "                        # grab target values from this dataframe\n",
    "                        controltarget_vals_list = list(controlsess_df[kwargs['target']].to_numpy())\n",
    "                        control_mask = mask\n",
    "\n",
    "                    save_predictions = kwargs.get('save_predictions_pseudo', kwargs['save_predictions'])\n",
    "\n",
    "                    # session for null dist\n",
    "                    ys_wmask = [controltarget_vals_list[m] for m in np.squeeze(np.where(control_mask))]\n",
    "                    Xs_wmask = [Xs[m] for m in np.squeeze(np.where(control_mask))]\n",
    "\n",
    "                if sample_pseudo_count > 1:\n",
    "                    print(f'sampled pseudo sessions {sample_pseudo_count} times to ensure valid target')\n",
    "\n",
    "            else:\n",
    "                control_mask = mask\n",
    "                save_predictions = kwargs['save_predictions']\n",
    "\n",
    "                # original session\n",
    "                ys_wmask = [target_vals_list[m] for m in np.squeeze(np.where(mask))]\n",
    "                Xs_wmask = [Xs[m] for m in np.squeeze(np.where(mask))]\n",
    "\n",
    "                if kwargs['estimator'] == sklm.LogisticRegression and (not logisticreg_criteria(ys_wmask)):\n",
    "                    print(f'target failed logistic regression criteria for region {region} and pseudo_id {pseudo_id}')\n",
    "                    continue\n",
    "\n",
    "            for i_run in range(kwargs['n_runs']):\n",
    "\n",
    "                rng_seed = i_run\n",
    "\n",
    "                fit_result = decode_cv(\n",
    "                    ys=ys_wmask,\n",
    "                    Xs=Xs_wmask,\n",
    "                    estimator=kwargs['estimator'],\n",
    "                    use_openturns=kwargs['use_openturns'],\n",
    "                    target_distribution=target_distribution,\n",
    "                    balanced_continuous_target=kwargs['balanced_continuous_target'],\n",
    "                    estimator_kwargs=kwargs['estimator_kwargs'],\n",
    "                    hyperparam_grid=kwargs['hyperparam_grid'],\n",
    "                    save_binned=kwargs['save_binned'] if pseudo_id == -1 else False,\n",
    "                    save_predictions=save_predictions,\n",
    "                    shuffle=kwargs['shuffle'],\n",
    "                    balanced_weight=kwargs['balanced_weight'],\n",
    "                    rng_seed=rng_seed,\n",
    "                )\n",
    "                fit_result['mask'] = mask\n",
    "                fit_result['mask_trials_and_targets'] = [trials_mask, target_mask]\n",
    "                fit_result['mask_diagnostics'] = kwargs['trials_mask_diagnostics']\n",
    "                fit_result['df'] = trials_df if pseudo_id == -1 else controlsess_df\n",
    "                fit_result['pseudo_id'] = pseudo_id\n",
    "                fit_result['run_id'] = i_run\n",
    "                fit_result['cluster_uuids'] = cl_uuids_used\n",
    "                fit_results.append(fit_result)\n",
    "\n",
    "            filename = save_region_results(\n",
    "                    fit_result=fit_results,\n",
    "                    pseudo_id=pseudo_id,\n",
    "                    subject=metadata['subject'],\n",
    "                    eid=metadata['eid'],\n",
    "                    probe=metadata['probe_name'],\n",
    "                    region=region,\n",
    "                    n_units=n_units,\n",
    "                    save_path=save_path\n",
    "            )\n",
    "\n",
    "            filenames.append(filename)\n",
    "            \n",
    "    filenames_by_eids.update({eid: filenames})\n",
    "            \n",
    "    res = []\n",
    "    for filename in filenames:\n",
    "        with open(filename, 'rb') as f:\n",
    "            res.append(pickle.load(f))\n",
    "\n",
    "    true_score = res[0][\"fit\"][0][\"scores_test_full\"]\n",
    "    null_median = np.median([null[\"fit\"][0][\"scores_test_full\"] for null in res[1:]])\n",
    "    adj_acc = true_score - null_median\n",
    "    metrics_by_eids.update({eid: [true_score, null_median, adj_acc]})\n",
    "\n",
    "    print(f'Finished eid: {metadata[\"eid\"]}')\n",
    "    print(metrics_by_eids)\n",
    "    np.save(save_path, metrics_by_eids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148f37aa-5810-496d-afc1-7c6e4465dee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bca4ecd-c13f-442c-997f-1253d6161f77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
